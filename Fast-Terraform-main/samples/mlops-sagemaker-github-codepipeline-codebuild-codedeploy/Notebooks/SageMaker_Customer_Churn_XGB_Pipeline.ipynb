{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Building and Deployment using SageMaker Workflow Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook we show how to use Amazon SageMaker to develop, train, tune and deploy a XGBoost model. Sythetic customer churn data is used. \n",
    "\n",
    "The data is in AWS public S3 bucket: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt\n",
    "\n",
    "Sklearn Processor is used to process the raw data.\n",
    "\n",
    "* XGBoost https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html?highlight=xgboost\n",
    "* Doc https://sagemaker.readthedocs.io/en/stable/using_sklearn.html\n",
    "* SDK https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html\n",
    "* boto3 https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#client\n",
    " \n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()  # this could also be a hard-coded bucket name\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(region)\n",
    "role = get_execution_role()\n",
    "\n",
    "project_name = \"test_pro\"\n",
    "project_id = \"test_id\"\n",
    "#model_package_group_name = project_name\n",
    "print(f\"sagemaker role arn <{role}>\")\n",
    "\n",
    "assert(len(project_name) <= 15 ) # the project name should not have more than 15 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data to S3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw data from the public S3 bucket to your own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt s3://{bucket}/sagemaker/DEMO-xgboost-churn/data/RawData.csv "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare script to be used by preprocessing job and model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create preprocessing script. This script will be used by SageMaker process job instance to preocess raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "\"\"\"Preprocess the customer churn dataset.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    input_data = args.input_data\n",
    "    print(input_data)\n",
    "    bucket = input_data.split(\"/\")[2]\n",
    "    key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "    fn = f\"{base_dir}/data/raw-data.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, fn)\n",
    "\n",
    "    logger.info(\"Reading downloaded data.\")\n",
    "\n",
    "    # read in csv\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    # drop the \"Phone\" feature column\n",
    "    df = df.drop([\"Phone\"], axis=1)\n",
    "\n",
    "    # Change the data type of \"Area Code\"\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "    # Drop several other columns\n",
    "    df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "    # Convert categorical variables into dummy/indicator variables.\n",
    "    model_data = pd.get_dummies(df)\n",
    "\n",
    "    # Create one binary classification target column\n",
    "    model_data = pd.concat(\n",
    "        [\n",
    "            model_data[\"Churn?_True.\"],\n",
    "            model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Split the data\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        model_data.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(model_data)), int(0.9 * len(model_data))],\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(train_data).to_csv(\n",
    "        f\"{base_dir}/train/train.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(validation_data).to_csv(\n",
    "        f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test_data).to_csv(\n",
    "        f\"{base_dir}/test/test.csv\", header=False, index=False\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "\"\"\"Evaluation script for measuring model accuracy.\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# May need to import additional metrics depending on what you are measuring.\n",
    "# See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\"..\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    print(\"Loading test input data\")\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Creating classification evaluation report\")\n",
    "    acc = accuracy_score(y_test, predictions.round())\n",
    "    auc = roc_auc_score(y_test, predictions.round())\n",
    "\n",
    "    # The metrics reported can change based on the model used, but it must be a specific name per (https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": acc,\n",
    "                \"standard_deviation\" : \"NaN\"\n",
    "            },\n",
    "            \"auc\" : {\n",
    "                \"value\" : auc,\n",
    "                \"standard_deviation\": \"NaN\"\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    evaluation_output_path = '/opt/ml/processing/evaluation/evaluation.json'\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Building Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pipeline input parameters are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"Approved\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "\n",
    "input_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value=f\"s3://{sagemaker_session.default_bucket()}/sagemaker/DEMO-xgboost-churn/data/RawData.csv\",  # Change this to point to the s3 location of your raw input data.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name=\"CustomerChurnPackageGroup\"  # Choose any name\n",
    "#pipeline_name=\"CustomerChurnDemoPipe2\"  # \n",
    "base_job_prefix=\"CustomerChurn\"  # Choose any name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime,gmtime\n",
    "pipeline_name = 'CustomerChurn-Pipe-' + strftime(\"%M%S\", gmtime())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below defines a SageMaker model buidling pipeline using workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing step for feature engineering\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=f\"{base_job_prefix}/sklearn-CustomerChurn-preprocess\",  # choose any name\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "step_process = ProcessingStep(\n",
    "    name=\"CustomerChurnProcess\",  # choose any name\n",
    "    processor=sklearn_processor,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=(\"preprocess.py\"),\n",
    "    job_arguments=[\"--input-data\", input_data],\n",
    ")\n",
    "# Training step for generating model artifacts\n",
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/CustomerChurnTrain\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",  # we are using the Sagemaker built in xgboost algorithm\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"{base_job_prefix}/CustomerChurn-train\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")\n",
    "step_train = TrainingStep(\n",
    "    name=\"CustomerChurnTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "# Processing step for evaluation\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_prefix}/script-CustomerChurn-eval\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"CustomerChurnEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=(\"evaluate.py\"),\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "# Register model step that will be conditionally executed\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "# Register model step that will be conditionally executed\n",
    "step_register = RegisterModel(\n",
    "    name=\"CustomerChurnRegisterModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "# Condition step for evaluating model quality and branching execution\n",
    "cond_lte = ConditionGreaterThanOrEqualTo(  # You can change the condition here\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.accuracy.value\",  # This should follow the structure of your report_dict defined in the evaluate.py file.\n",
    "    ),\n",
    "    right=0.8,  # You can change the threshold here\n",
    ")\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CustomerChurnAccuracyCond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# Pipeline instance\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError, ValidationError\n",
    "\n",
    "try:\n",
    "    response = pipeline.create(role_arn=role)\n",
    "except ClientError as e:\n",
    "    error = e.response[\"Error\"]\n",
    "    if error[\"Code\"] == \"ValidationError\" and \"Pipeline names must be unique within\" in error[\"Message\"]:\n",
    "        print(error[\"Message\"])\n",
    "        response = pipeline.describe()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "sm_client.add_tags(\n",
    "    ResourceArn=pipeline_arn,\n",
    "    Tags=[\n",
    "        {'Key': 'sagemaker:project-name', 'Value': project_name },\n",
    "        {'Key': 'sagemaker:project-id', 'Value': project_id }\n",
    "    ]\n",
    ")\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start()\n",
    "\n",
    "pipeline_execution_arn = start_response.arn\n",
    "print(pipeline_execution_arn)\n",
    "\n",
    "while True:\n",
    "    resp = sm_client.describe_pipeline_execution(PipelineExecutionArn=pipeline_execution_arn)\n",
    "    if resp['PipelineExecutionStatus'] == 'Executing':\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['PipelineExecutionStatus'], pipeline_execution_arn)\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approve the model to kick-off the deployment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all packages and select the latest one\n",
    "packages = sm_client.list_model_packages(ModelPackageGroupName=model_package_group_name)['ModelPackageSummaryList']\n",
    "packages = sorted(packages, key=lambda x: x['CreationTime'], reverse=True)\n",
    "\n",
    "latest_model_package_arn = packages[0]['ModelPackageArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.list_model_packages(ModelPackageGroupName=model_package_group_name)['ModelPackageSummaryList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latest_model_package_arn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approve model\n",
    "Approval permission controlled by IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_response = sm_client.update_model_package(\n",
    "    ModelPackageArn=latest_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Approved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details=sm_client.describe_model_package(ModelPackageName=latest_model_package_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=model_details['InferenceSpecification']['Containers'][0]['ModelDataUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=model_details['InferenceSpecification']['Containers'][0]['Image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_data} ./pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "# open file\n",
    "file = tarfile.open('./pipeline_model/model.tar.gz')\n",
    "  \n",
    "# extracting file\n",
    "file.extractall('./pipeline_model')\n",
    "  \n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction using Local Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is created in the SageMaker notebook instance instead of using SageMaker endpoint instance. The local model in the notebook instance is used to check predictions within the notebook. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Install XGboost library on the Notebook instance if not installed. \n",
    "2. Load the trained model.\n",
    "3. Use test data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "loaded_model = joblib.load(\"./pipeline_model/xgboost-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=test_data.iloc[0:1,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(xgtest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and deploy an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime,gmtime\n",
    "model_name = 'CustomerChurn-model-' + strftime(\"%M%S\", gmtime())\n",
    "model_version_arn=latest_model_package_arn\n",
    "\n",
    "print(\"Model name : {}\".format(model_name))\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,  \n",
    "    PrimaryContainer = {\n",
    "        \"ModelPackageName\": model_version_arn,\n",
    "        \n",
    "    }\n",
    "    \n",
    ") \n",
    "print(\"Model arn : {}\".format(create_model_response[\"ModelArn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create endpointconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'Test-EndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.t2.medium',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'Test-endpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.describe_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_response(client, endpoint_name, poll_interval=30):\n",
    "    ### Wait until the job finishes\n",
    "    status = 'Creating'\n",
    "    while(status == 'Creating'):\n",
    "        response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response['EndpointStatus']\n",
    "        print('Creating job is still in status: {} ...'.format(status))\n",
    "        if status == 'Failed':\n",
    "            message = response['FailureReason']\n",
    "            logging.info('Endpoint Creation failed with the following error: {}'.format(message))\n",
    "            print('Endpoint failed with the following error: {}'.format(message))\n",
    "            raise Exception('Creating Endpoint failed')\n",
    "        logging.info(\"Creating job is still in status: \" + status)\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "    if status == 'InService':\n",
    "        logging.info(\"Creating job ended with status: \" + status)\n",
    "        print('Creating job ended with status: {}'.format(status))\n",
    "    else:\n",
    "        raise Exception('Creating job stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_response(sm_client, endpoint_name, poll_interval=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test.csv',header=None)\n",
    "testdata1=test_data.iloc[0:1,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "Endpoint_name=endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# csv serialization\n",
    "\n",
    "prediction = runtime.invoke_endpoint(\n",
    "    EndpointName=Endpoint_name,\n",
    "    Body=testdata1.to_csv(header=False, index=False).encode(\"utf-8\"),\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept= \"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.18987475335597992,0.9903860092163086,0.003180732252076268,0.010586234740912914,0.5162394046783447'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "test_data=pd.read_csv('test.csv',header=None)\n",
    "testdata1=test_data.iloc[0:5,1:]\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "Endpoint_name= ''  #<your endpoint name> # update to your own endpoint name\n",
    "\n",
    "prediction = runtime.invoke_endpoint(\n",
    "    EndpointName=Endpoint_name,\n",
    "    Body=testdata1.to_csv(header=False, index=False).encode(\"utf-8\"),\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept= \"text/csv\",\n",
    ")\n",
    "\n",
    "print(prediction[\"Body\"].read())"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
