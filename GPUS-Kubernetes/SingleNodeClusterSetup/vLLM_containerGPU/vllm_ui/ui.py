"""
vLLM Web UI with Prometheus Metrics

A FastAPI-based web interface for interacting with a vLLM backend server.
This application provides:
- A simple HTML chat interface for LLM inference
- Prometheus metrics for monitoring request latency and token usage
- Async HTTP client for efficient vLLM backend communication

Architecture:
    User Browser <-> This UI (FastAPI) <-> vLLM Backend (OpenAI-compatible API)

The UI connects to vLLM's OpenAI-compatible /v1/chat/completions endpoint
and tracks metrics for Prometheus/Grafana monitoring.

Environment Variables:
    VLLM_URL: Base URL of the vLLM server (default: "http://127.0.0.1:8000")
    MODEL_ID: Model identifier for API requests (default: "Qwen/Qwen2.5-3B-Instruct")

Metrics Exposed:
    - ui_http_requests_total: Counter of all HTTP requests by status code
    - ui_http_request_latency_seconds: Histogram of request latencies by path
    - ui_tokens_in_total: Counter of prompt tokens sent to the model
    - ui_tokens_out_total: Counter of completion tokens generated
"""

import os
import time

from fastapi import FastAPI, Request, Form, Response
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import httpx

# Prometheus client library for exposing metrics
from prometheus_client import (
    Counter,
    Histogram,
    generate_latest,
    CONTENT_TYPE_LATEST,
)

# =============================================================================
# Configuration
# =============================================================================

# URL of the vLLM backend server (OpenAI-compatible API)
VLLM_URL = os.environ.get("VLLM_URL", "http://127.0.0.1:8000")

# Model ID to use for chat completions (must match model loaded in vLLM)
MODEL_ID = os.environ.get("MODEL_ID", "Qwen/Qwen2.5-3B-Instruct")

print(">>> VLLM_URL:", VLLM_URL)
print(">>> MODEL_ID:", MODEL_ID)

# =============================================================================
# Prometheus Metrics Definitions
# =============================================================================

# Counter: Track total HTTP requests grouped by status code
# Useful for monitoring error rates (4xx, 5xx) vs success (2xx)
REQUEST_COUNT = Counter(
    "ui_http_requests_total",
    "Total HTTP requests",
    ["status"],  # Label for HTTP status code
)

# Histogram: Track request latency distribution by endpoint path
# Histograms enable percentile calculations (p50, p95, p99)
REQUEST_LATENCY = Histogram(
    "ui_http_request_latency_seconds",
    "Latency of HTTP requests",
    ["path"],  # Label for the request path
)

# Counter: Track total prompt tokens sent to the model
# Useful for cost estimation and usage monitoring
TOKENS_IN = Counter(
    "ui_tokens_in_total",
    "Total prompt tokens sent to model",
)

# Counter: Track total completion tokens generated by the model
# Useful for output volume monitoring
TOKENS_OUT = Counter(
    "ui_tokens_out_total",
    "Total completion tokens generated",
)

# =============================================================================
# FastAPI Application Setup
# =============================================================================

app = FastAPI()

# Mount static files directory for CSS, JavaScript, images
app.mount("/static", StaticFiles(directory="static"), name="static")

# Configure Jinja2 templates for HTML rendering
templates = Jinja2Templates(directory="templates")


# =============================================================================
# Middleware for Global Metrics Collection
# =============================================================================

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    """
    HTTP middleware that instruments ALL requests with metrics.

    This middleware runs on every incoming request and:
    1. Records the start time before processing
    2. Passes the request to the next handler
    3. Calculates the total request duration
    4. Updates Prometheus metrics with latency and status code

    Using middleware means you don't need to manually instrument each route.

    Args:
        request: The incoming HTTP request
        call_next: Function to call the next middleware/route handler

    Returns:
        Response: The HTTP response from the route handler
    """
    # Record start time for latency measurement
    start = time.time()

    # Process the request through the route handler
    response = await call_next(request)

    # Calculate request duration
    duration = time.time() - start

    # Record latency in histogram (grouped by URL path)
    REQUEST_LATENCY.labels(request.url.path).observe(duration)

    # Increment request counter (grouped by HTTP status code)
    REQUEST_COUNT.labels(status=str(response.status_code)).inc()

    return response


# =============================================================================
# Prometheus Metrics Endpoint
# =============================================================================

@app.get("/metrics")
def metrics():
    """
    Prometheus metrics scrape endpoint.

    This endpoint exposes all registered Prometheus metrics in the standard
    text exposition format. Configure your Prometheus server to scrape this
    endpoint (typically every 15-30 seconds).

    Security Note: In production, restrict access to this endpoint using
    network policies, mTLS, or authentication middleware.

    Returns:
        Response: Prometheus metrics in text exposition format
    """
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


# =============================================================================
# Web UI Endpoint
# =============================================================================

@app.get("/", response_class=HTMLResponse)
async def get_ui(request: Request):
    """
    Serve the main chat interface HTML page.

    Renders the index.html template with the configured model ID
    displayed in the UI.

    Args:
        request: The incoming HTTP request (required by Jinja2)

    Returns:
        HTMLResponse: Rendered HTML chat interface
    """
    return templates.TemplateResponse(
        "index.html",
        {"request": request, "model_id": MODEL_ID},
    )


# =============================================================================
# LLM Generation API Endpoint
# =============================================================================

@app.post("/api/generate")
async def generate(prompt: str = Form(...)):
    """
    Generate a response from the LLM via the vLLM backend.

    This endpoint:
    1. Receives a user prompt from the web form
    2. Sends it to the vLLM backend using OpenAI-compatible API format
    3. Extracts the response and token usage statistics
    4. Updates Prometheus metrics for token tracking
    5. Returns the response with latency information

    Args:
        prompt: User's input text from the chat form

    Returns:
        dict: {
            "response": "LLM generated text...",
            "latency_ms": 123.4,
            "usage": {"prompt_tokens": 10, "completion_tokens": 50, ...}
        }

    Raises:
        500 error: If communication with vLLM backend fails
    """
    # Construct the request payload in OpenAI chat completion format
    payload = {
        "model": MODEL_ID,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 512,      # Limit response length
        "temperature": 0.2,     # Lower = more deterministic responses
    }

    # Record start time for latency tracking
    start = time.monotonic()

    try:
        # Use async HTTP client with 60-second timeout for LLM inference
        # vLLM exposes an OpenAI-compatible API at /v1/chat/completions
        async with httpx.AsyncClient(timeout=60.0) as client:
            r = await client.post(f"{VLLM_URL}/v1/chat/completions", json=payload)

        # Calculate request latency in milliseconds
        elapsed_ms = (time.monotonic() - start) * 1000.0

        # Raise exception for HTTP error status codes (4xx, 5xx)
        r.raise_for_status()

        # Parse the JSON response
        data = r.json()

        # Extract the assistant's response text
        text = data["choices"][0]["message"]["content"]

        # Extract token usage statistics (may not be present in all responses)
        usage = data.get("usage", {})

        # Update Prometheus token counters for usage monitoring
        TOKENS_IN.inc(usage.get("prompt_tokens", 0))
        TOKENS_OUT.inc(usage.get("completion_tokens", 0))

    except Exception as e:
        # Return error response if vLLM communication fails
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to contact vLLM: {e}"},
        )

    # Return successful response with text, latency, and usage stats
    return {
        "response": text.strip(),
        "latency_ms": round(elapsed_ms, 1),
        "usage": usage,
    }


# =============================================================================
# Local Development Server
# =============================================================================

if __name__ == "__main__":
    # Run with uvicorn for local development
    # In production, use: uvicorn ui:app --host 0.0.0.0 --port 8080
    import uvicorn
    uvicorn.run("ui:app", host="0.0.0.0", port=8080, reload=True)
