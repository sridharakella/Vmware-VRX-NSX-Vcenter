#!/usr/bin/env python3
"""
Load test your UI service (not vLLM directly) to capture Prometheus metrics
"""
import asyncio
import aiohttp
import time
import random
import statistics
from typing import List

UI_ENDPOINT = "http://localhost:8080/api/generate"

PROMPTS = [
    "Explain REST APIs in 100 words",
    "List 25 European capitals",
    "Describe how DNS works",
    "What is containerization?",
    "Compare Git and SVN",
    "Explain neural networks simply",
    "List cloud service providers",
    "Describe the OSI model",
    "What is continuous deployment?",
    "Explain database indexing"
]

async def send_request(session, request_id: int) -> dict:
    """Send request to UI service"""
    prompt = random.choice(PROMPTS)
    
    form_data = aiohttp.FormData()
    form_data.add_field('prompt', prompt)
    
    start_time = time.time()
    try:
        async with session.post(UI_ENDPOINT, data=form_data) as resp:
            data = await resp.json()
            latency = time.time() - start_time
            
            usage = data.get("usage", {})
            return {
                "request_id": request_id,
                "latency": latency,
                "status": resp.status,
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0),
                "ui_latency_ms": data.get("latency_ms", 0)
            }
    except Exception as e:
        return {
            "request_id": request_id,
            "latency": time.time() - start_time,
            "status": 500,
            "error": str(e)
        }

async def run_load_test(num_requests: int, concurrency: int):
    """Execute load test"""
    print(f"ðŸš€ Starting load test")
    print(f"   Target: {UI_ENDPOINT}")
    print(f"   Requests: {num_requests}, Concurrency: {concurrency}\n")
    
    results = []
    connector = aiohttp.TCPConnector(limit=concurrency)
    
    test_start = time.time()
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [send_request(session, i) for i in range(num_requests)]
        results = await asyncio.gather(*tasks)
    test_duration = time.time() - test_start
    
    # Filter successful requests
    successful = [r for r in results if "error" not in r]
    failed = len(results) - len(successful)
    
    print("=" * 60)
    print("ðŸ“Š LOAD TEST RESULTS")
    print("=" * 60)
    print(f"Total Requests:     {len(results)}")
    print(f"Successful:         {len(successful)} ({len(successful)/len(results)*100:.1f}%)")
    print(f"Failed:             {failed}")
    print(f"Test Duration:      {test_duration:.2f}s")
    print(f"Requests/sec:       {len(results)/test_duration:.2f}")
    
    if successful:
        latencies = [r["latency"] for r in successful]
        tokens = [r["total_tokens"] for r in successful]
        tokens_per_sec = [
            r["total_tokens"] / r["latency"] 
            for r in successful if r["latency"] > 0
        ]
        
        print(f"\n{'LATENCY METRICS':-^60}")
        print(f"P50 (median):       {statistics.quantiles(latencies, n=100)[49]:.3f}s")
        print(f"P95:                {statistics.quantiles(latencies, n=100)[94]:.3f}s")
        print(f"P99:                {statistics.quantiles(latencies, n=100)[98]:.3f}s")
        print(f"Average:            {statistics.mean(latencies):.3f}s")
        print(f"Min:                {min(latencies):.3f}s")
        print(f"Max:                {max(latencies):.3f}s")
        
        print(f"\n{'TOKEN METRICS':-^60}")
        print(f"Total tokens:       {sum(tokens):,}")
        print(f"Avg tokens/request: {statistics.mean(tokens):.1f}")
        print(f"Avg tokens/sec:     {statistics.mean(tokens_per_sec):.2f}")
        
        print("\nðŸ’¡ Now check your Grafana dashboard to see these metrics!")
        print("   The spikes in P95/P99 show the effect of concurrent requests")

if __name__ == "__main__":
    # Test configurations - start small, then increase
    
    # Light load
    # asyncio.run(run_load_test(num_requests=20, concurrency=5))
    
    # Medium load (good for demo)
    asyncio.run(run_load_test(num_requests=50, concurrency=10))
    
    # Heavy load (watch your metrics spike!)
    # asyncio.run(run_load_test(num_requests=100, concurrency=20))
